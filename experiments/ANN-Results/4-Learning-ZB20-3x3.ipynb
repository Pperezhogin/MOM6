{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from helpers.ann_tools import *\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANNs on staggered grids with stencil 3x3\n",
    "In this notebook, we enlarge spatial stencil to 3x3, but also simplify features: all input features are on the same grid. Also, we train two distinct networks for corner and center points because it allows to directly account for rotation symmetry.\n",
    "\n",
    "![figure](Staggered_grid_3x3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZB20 as ANN\n",
    "Note the following important changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZB20_Txy(x):\n",
    "    '''\n",
    "    x is the vector containing three input features: sh_xy(corner), sh_xx (interpolated to corner), vort_xy(corner)\n",
    "    They are stacked together to form 3*9 input features\n",
    "    9 consecutive points describe the stencil 3x3 where \n",
    "    the fast index corresponds to zonal (\"x\") direction (as in MOM6) \n",
    "    \n",
    "    This function is NN, being a part of mapping T = delta^2 * ||X||^2 * NN(X / ||X||)\n",
    "    with norm being second norm, and delta is the grid step.\n",
    "    \n",
    "    The output vector y contains values Txy, where\n",
    "    T = [0 Txy; Txy, 0] and\n",
    "    du/dt = -div(T)\n",
    "    \n",
    "    Note that the leftmost dimension is batch, i.e.\n",
    "    x(:,12), y(:,3)\n",
    "    '''\n",
    "    if x.shape[1] != 27:\n",
    "        print('Error')\n",
    "        return\n",
    "    \n",
    "    # Vorticity in corner point, corresponding to prediction point\n",
    "    # 9*2 means that vort_xy is the last fature in input vector\n",
    "    vort_xy = x[:,4+9*2].reshape(-1,1)\n",
    "    # Again in the same corner point, but now it is the second feature, so 1*9\n",
    "    sh_xx = x[:,4+9*1].reshape(-1,1)\n",
    "    \n",
    "    Txy = vort_xy * sh_xx\n",
    "    \n",
    "    y = torch.zeros(x.shape[0],1)\n",
    "    y[:,0:1] = Txy\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZB20_Txx_Tyy(x):\n",
    "    '''\n",
    "    x is the vector containing three input features: \n",
    "    sh_xy(interpolated to center), sh_xx (center), vort_xy(interpolated_to center)\n",
    "    They are stacked together to form 3*9 input features\n",
    "    9 consecutive points describe the stencil 3x3 where \n",
    "    the fast index corresponds to zonal (\"x\") direction (as in MOM6) \n",
    "    \n",
    "    This function is NN, being a part of mapping T = delta^2 * ||X||^2 * NN(X / ||X||)\n",
    "    with norm being second norm, and delta is the grid step.\n",
    "    \n",
    "    The output vector y contains values Tdd, Ttr, where\n",
    "    T = [Tdd+Ttr; 0; 0, -Tdd+Ttr]\n",
    "    du/dt = -div(T)\n",
    "    \n",
    "    Note that the leftmost dimension is batch, i.e.\n",
    "    x(:,12), y(:,3)\n",
    "    '''\n",
    "    if x.shape[1] != 27:\n",
    "        print('Error')\n",
    "        return\n",
    "    \n",
    "    vort_xy = x[:,4+9*2].reshape(-1,1)\n",
    "    sh_xx = x[:,4+9*1].reshape(-1,1)\n",
    "    sh_xy = x[:,4].reshape(-1,1)\n",
    "    \n",
    "    y = torch.zeros(x.shape[0],2)\n",
    "    y[:,0:1] = 0.5 * (sh_xx**2 + (vort_xy-sh_xy)**2)\n",
    "    y[:,1:2] = 0.5 * (sh_xx**2 + (vort_xy+sh_xy)**2)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = torch.randn(1)\n",
    "D_hat = torch.randn(1)\n",
    "zeta = torch.randn(1)\n",
    "x = torch.tensor([[D]*9 + [D_hat]*9 + [zeta]*9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tdd = -zeta*D\n",
    "Ttr = 0.5*(D**2+D_hat**2+zeta**2)\n",
    "Txy = zeta * D_hat\n",
    "Txx = Ttr + Tdd\n",
    "Tyy = Ttr - Tdd\n",
    "T = torch.tensor([[Txx, Tyy, Txy]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0402, 0.0592]]) tensor([[0.0108]])\n"
     ]
    }
   ],
   "source": [
    "print(ZB20_Txx_Tyy(x), ZB20_Txy(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0402, 0.0592, 0.0108]])\n"
     ]
    }
   ],
   "source": [
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training ANN on ZB20 data\n",
    "Here we generate input features from Gaussian distribution and normalize by their L2 norm.\n",
    "We will do the same on inferent. The output normalization constant is needed only for normalization of MSE loss. We will pass it to the loss function, but for convenience will not use as part of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_on_a_unit_sphere(Nsamples, Nfeatures):\n",
    "    x = torch.randn(Nsamples, Nfeatures)\n",
    "    norm = torch.sqrt((x**2).sum(dim=1, keepdims=True))\n",
    "    return x / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Txy part of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsamples = 10000000\n",
    "x_train = noise_on_a_unit_sphere(Nsamples, 27)\n",
    "y_train = ZB20_Txy(x_train)\n",
    "\n",
    "x_test = noise_on_a_unit_sphere(Nsamples, 27)\n",
    "y_test = ZB20_Txy(x_test)\n",
    "\n",
    "output_norm = float(torch.sqrt((y_train**2).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = ANN(layer_sizes=[27,20,1], output_norm=output_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts on device cpu, number of samples 10000000\n",
      "[1/20] [14.12/268.23] Loss: [0.027752, 0.002781]\n",
      "[2/20] [13.74/250.73] Loss: [0.001920, 0.001005]\n",
      "[3/20] [13.94/236.86] Loss: [0.000887, 0.000921]\n",
      "[4/20] [13.96/223.03] Loss: [0.000834, 0.000857]\n",
      "[5/20] [14.05/209.41] Loss: [0.000829, 0.000852]\n",
      "[6/20] [13.86/195.22] Loss: [0.000825, 0.000820]\n",
      "[7/20] [13.91/181.21] Loss: [0.000824, 0.000842]\n",
      "[8/20] [14.11/167.53] Loss: [0.000822, 0.000811]\n",
      "[9/20] [22.58/164.10] Loss: [0.000821, 0.000791]\n",
      "[10/20] [15.33/149.60] Loss: [0.000820, 0.001142]\n",
      "[11/20] [14.31/134.11] Loss: [0.000717, 0.000720]\n",
      "[12/20] [14.12/118.68] Loss: [0.000717, 0.000712]\n",
      "[13/20] [14.04/103.42] Loss: [0.000717, 0.000711]\n",
      "[14/20] [13.82/88.24] Loss: [0.000716, 0.000718]\n",
      "[15/20] [14.04/73.31] Loss: [0.000715, 0.000714]\n",
      "[16/20] [13.95/58.47] Loss: [0.000702, 0.000703]\n",
      "[17/20] [13.99/43.74] Loss: [0.000702, 0.000702]\n",
      "[18/20] [13.96/29.09] Loss: [0.000700, 0.000701]\n",
      "[19/20] [13.99/14.52] Loss: [0.000700, 0.000701]\n",
      "[20/20] [13.97/0.00] Loss: [0.000700, 0.000701]\n"
     ]
    }
   ],
   "source": [
    "train(ann, x_train, y_train, x_test, y_test, 20, 1000, 1e-3, print_frequency=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R2(target, pred):\n",
    "    return float(1 - ((target-pred)**2).sum() / (target**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999299943447113"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2(y_test, ann(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0616],\n",
      "        [ 0.0092],\n",
      "        [-0.0153],\n",
      "        [-0.0035],\n",
      "        [ 0.0701]])\n",
      "tensor([[-0.0615],\n",
      "        [ 0.0103],\n",
      "        [-0.0144],\n",
      "        [-0.0035],\n",
      "        [ 0.0692]])\n"
     ]
    }
   ],
   "source": [
    "print(ann(x_test[0:5]).data)\n",
    "print(ZB20_Txy(x_test[0:5]).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test =  [ 1.125071   -0.66410357  0.03942366 -1.1223322   1.3186445   1.4001092\n",
      "  0.9710138   1.4577577  -0.58660185  1.0608376   0.5968267  -0.35395458\n",
      " -0.7077333   0.8298185   0.6564303  -0.7864476   0.2546004  -1.2482073\n",
      "  2.5276103  -1.097128    1.9325942  -0.06448472 -0.21903196 -2.2244322\n",
      "  0.532548   -1.0340395  -2.4342656 ]\n",
      "y_test =  [-0.14551666]\n"
     ]
    }
   ],
   "source": [
    "export_ANN(ann, input_norms=torch.ones(27), output_norms=torch.ones(1), \n",
    "           filename='trained_models/ANN_Txy_ZB-small.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Txx, Tyy part of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsamples = 10000000\n",
    "x_train = noise_on_a_unit_sphere(Nsamples, 27)\n",
    "y_train = ZB20_Txx_Tyy(x_train)\n",
    "\n",
    "x_test = noise_on_a_unit_sphere(Nsamples, 27)\n",
    "y_test = ZB20_Txx_Tyy(x_test)\n",
    "\n",
    "output_norm = float(torch.sqrt((y_train**2).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = ANN(layer_sizes=[27,20,2], output_norm=output_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts on device cpu, number of samples 10000000\n",
      "[1/20] [15.65/297.43] Loss: [0.040287, 0.006444]\n",
      "[2/20] [15.32/278.80] Loss: [0.006143, 0.006009]\n",
      "[3/20] [14.72/258.97] Loss: [0.005661, 0.004934]\n",
      "[4/20] [15.10/243.22] Loss: [0.004255, 0.003412]\n",
      "[5/20] [14.74/226.62] Loss: [0.003443, 0.003418]\n",
      "[6/20] [14.97/211.19] Loss: [0.003442, 0.003495]\n",
      "[7/20] [16.52/198.76] Loss: [0.003439, 0.003432]\n",
      "[8/20] [14.97/182.99] Loss: [0.003435, 0.003428]\n",
      "[9/20] [14.57/166.91] Loss: [0.002855, 0.002320]\n",
      "[10/20] [14.64/151.21] Loss: [0.002287, 0.002411]\n",
      "[11/20] [15.69/136.55] Loss: [0.002183, 0.002181]\n",
      "[12/20] [15.21/121.40] Loss: [0.002183, 0.002187]\n",
      "[13/20] [17.03/107.22] Loss: [0.002183, 0.002188]\n",
      "[14/20] [18.74/93.37] Loss: [0.002182, 0.002179]\n",
      "[15/20] [15.91/77.93] Loss: [0.002182, 0.002185]\n",
      "[16/20] [14.70/62.12] Loss: [0.002168, 0.002172]\n",
      "[17/20] [15.56/46.60] Loss: [0.002168, 0.002171]\n",
      "[18/20] [16.99/31.23] Loss: [0.002166, 0.002170]\n",
      "[19/20] [15.84/15.62] Loss: [0.002166, 0.002170]\n",
      "[20/20] [15.13/0.00] Loss: [0.002166, 0.002170]\n"
     ]
    }
   ],
   "source": [
    "train(ann, x_train, y_train, x_test, y_test, 20, 1000, 1e-3, print_frequency=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9978306293487549"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2(y_test, ann(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test =  [ 0.5276723  -0.56469125 -0.02669321 -1.3318404  -0.2089864   0.4190437\n",
      "  1.7333885  -0.95261437  0.36575687  0.5111949  -1.069809   -0.25853646\n",
      " -0.61322135  1.0860585   0.56493235  0.3940468  -0.28736952  1.063206\n",
      "  0.56260747 -1.4229612   1.4992539   0.8398847   2.1048677  -0.0810979\n",
      " -1.1078862  -0.46300948 -1.2692279 ]\n",
      "y_test =  [1.5305986 1.3409165]\n"
     ]
    }
   ],
   "source": [
    "export_ANN(ann, input_norms=torch.ones(27), output_norms=torch.ones(2), \n",
    "           filename='trained_models/ANN_Txx_Tyy_ZB-small-retrain.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment_13_Jul_2023",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
